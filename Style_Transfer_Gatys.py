# -*- coding: utf-8 -*-
"""neural_style_transfer_fast_VGGexperiments.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VKFpRTRyEn3oXyXdZ0LCMp3EjvXV7WNA

Based on https://www.tensorflow.org/tutorials/generative/style_transfer
"""

# Import necessary libraries
import tensorflow as tf
import IPython.display as display

import cv2 as cv
import matplotlib.pyplot as plt
import matplotlib as mpl
mpl.rcParams['figure.figsize'] = (12, 12)
mpl.rcParams['axes.grid'] = False

import numpy as np
import PIL.Image
import time
import functools
import time

from pprint import pprint
from random import randrange

# Tensor to image convert function
def tensor_to_image(tensor):
  tensor = tensor*255
  tensor = np.array(tensor, dtype=np.uint8)
  if np.ndim(tensor)>3:
    assert tensor.shape[0] == 1
    tensor = tensor[0]
  return PIL.Image.fromarray(tensor)

# Load image function
def load_img(path_to_img):
  max_dim = 512
  img = tf.io.read_file(path_to_img)
  img = tf.image.decode_image(img, channels=3)
  img = tf.image.convert_image_dtype(img, tf.float32)

  shape = tf.cast(tf.shape(img)[:-1], tf.float32)
  long_dim = max(shape)
  scale = max_dim / long_dim

  new_shape = tf.cast(shape * scale, tf.int32)

  img = tf.image.resize(img, new_shape)
  img = img[tf.newaxis, :]
  return img

# Show image function
def imshow(image, title=None):
  if len(image.shape) > 3:
    image = tf.squeeze(image, axis=0)

  plt.imshow(image)
  if title:
    plt.title(title)

# Build the model function 
def vgg_layers(layer_names):
  """ Creates a vgg model that returns a list of intermediate output values."""
  # Load our model. Load pretrained VGG, trained on imagenet data
  vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')
  vgg.trainable = False

  outputs = [vgg.get_layer(name).output for name in layer_names]

  model = tf.keras.Model([vgg.input], outputs)
  return model

# Gram Matrix
def gram_matrix(input_tensor):
  result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)
  input_shape = tf.shape(input_tensor)
  num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)
  return result/(num_locations)

class StyleContentModel(tf.keras.models.Model):
  def __init__(self, style_layers, content_layers):
    super(StyleContentModel, self).__init__()
    self.vgg = vgg_layers(style_layers + content_layers)
    self.style_layers = style_layers
    self.content_layers = content_layers
    self.num_style_layers = len(style_layers)
    self.vgg.trainable = False

  def call(self, inputs):
    "Expects float input in [0,1]"
    inputs = inputs*255.0
    preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)
    outputs = self.vgg(preprocessed_input)
    style_outputs, content_outputs = (outputs[:self.num_style_layers],
                                      outputs[self.num_style_layers:])

    style_outputs = [gram_matrix(style_output)
                     for style_output in style_outputs]

    content_dict = {content_name: value
                    for content_name, value
                    in zip(self.content_layers, content_outputs)}

    style_dict = {style_name: value
                  for style_name, value
                  in zip(self.style_layers, style_outputs)}

    return {'content': content_dict, 'style': style_dict}

def clip_0_1(image):
  return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)

def style_content_loss(outputs, style_targets, content_targets, num_style_layers, num_content_layers):
    style_outputs = outputs['style']
    content_outputs = outputs['content']
    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2) 
                           for name in style_outputs.keys()])
    style_loss *= style_weight / num_style_layers

    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) 
                             for name in content_outputs.keys()])
    content_loss *= content_weight / num_content_layers
    loss = style_loss + content_loss
    return loss

def GenerateRandomLayer(number_of_layers):
  model = tf.keras.applications.VGG19(include_top=False, weights='imagenet')
  model.trainable = False
  # We have not picked any layers yet
  layers = []
  layers_picked = 0
  # Number of layers in model
  no_layers = len(model.layers)

  while layers_picked <  number_of_layers :
    #print(layers_picked)
    # Pick a random layer
    layer_index = randrange(no_layers)
    won_layer = model.layers[layer_index]
    if "_conv" in won_layer.name:
      if won_layer.name in layers:
        continue
      else:
        layers.append(won_layer.name)
        layers_picked = layers_picked + 1

  return layers

def training_loop(image, epochs = 10, steps_per_epoch = 100):
  start = time.time()

  step = 0
  for n in range(epochs):
    for m in range(steps_per_epoch):
      step += 1
      loss = train_step(image)
      print(".", end='')
    #display.clear_output(wait=True)
    
    print("Train step: {}".format(step))
    
  image_list.append(tensor_to_image(image)  )
  
  display.display(tensor_to_image(image))
  image_list[-1].save("image.jpg")
  end = time.time()
  print("Total time: {:.1f}".format(end-start))
  print(f"Loss :{loss.numpy()}")

def train_step(image):
  with tf.GradientTape() as tape:
    outputs = extractor(image)
    loss = style_content_loss(outputs, style_targets, content_targets, num_style_layers, num_content_layers)
    loss += total_variation_weight*tf.image.total_variation(image)
    


  grad = tape.gradient(loss, image)
  
  opt.apply_gradients([(grad, image)])
  image.assign(clip_0_1(image))
  return loss

# Import images
content_path = tf.keras.utils.get_file('YellowLabradorLooking_new.jpg', 'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg')
style_path = tf.keras.utils.get_file('kandinsky5.jpg','https://storage.googleapis.com/download.tensorflow.org/example_images/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg')

image_list = []
layers_list = []

#content_image = load_img(content_path)
#style_image = load_img(style_path)
vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')
vgg.trainable=False

'''outputs = []
for layer in vgg.layers:
  if isinstance(layer, tf.keras.layers.Conv2D):
    outputs.append(vgg.get_layer(layer.name).output)

model = tf.keras.Model([vgg.input], outputs)
imgs = model(content_image)'''

'''img = imgs[5]
filters = img.shape[-1]
fig, axs = plt.subplots(filters,11)
fig.set_size_inches(50, 200)
for x in range(filters):
  axs[x].axis("off")
  axs[x].imshow(img[y,:,:,x],cmap="hot")

fig.savefig("lol")'''

# Number of experiments you want to run
experiments = 1
# Number of layers you want to generate
number_of_layers = 1
# Index for seperating the layers into content and style
seperation_index = 0

for experiment in range(experiments):
  print(f"Experiment : {experiment}")
  layers = GenerateRandomLayer(number_of_layers)
  # Choose content and style layers
  content_layers = ['block4_conv2']

  style_layers = ['block1_conv1',
                  'block2_conv1',
                  'block3_conv1', 
                  'block4_conv1']

  layers_list.append(content_layers+style_layers)
  print(f"Content Layers: {content_layers}")
  print(f"Style Layers: {style_layers}")

  num_content_layers = len(content_layers)
  num_style_layers = len(style_layers)
  extractor = StyleContentModel(style_layers, content_layers)
  # Load the 2 images
  
  style_image = load_img(style_path)
  total_variation_weight=30
  style_targets = extractor(style_image)['style']
  content_image = load_img(content_path)
  content_targets = extractor(content_image)['content']
  image = tf.Variable(content_image)
  # Create an optimizer
  opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)
  style_weight=1e30
  content_weight=1
  training_loop(image)
"""
def plot(images, numOfExpe, rows, cols, figTitle):
  nrows = rows
  ncols = cols
  fig, axs = plt.subplots(nrows,ncols)
  fig.suptitle(figTitle, fontsize=16, )
  fig.set_size_inches(15, 9)
  #images = tensor_to_image(images, show = False)
  epoch = 1 
  for x in range(nrows):
    for y in range(ncols):
      imagePerEpoch = images[x*cols + y]
      axs[x,y].axis("off")
      #axs[x,y].set_title(f"Content = {layers_list[x*cols + y][0:x*cols + y+1]}\n alpha ={content_weight} , beta ={style_weight} ", fontsize=10)
      axs[x,y].imshow(imagePerEpoch)
      epoch = epoch + 1
  fig.savefig("differentnumberofcontentlayers")

#plot(image_list, 4, 2, 2, "Different Number of Content Layers")

#Filters Output

layers= ["block1_conv1",
         "block2_conv2",
         "block3_conv1",
         "block4_conv2",
         "block5_conv2"]
models= {layers[0]:None,
         layers[1]:None,
         layers[2]:None,
         layers[3]:None,
         layers[4]:None}
generated_img = []
for layer_index in range(len(vgg.layers)):
  if vgg.get_layer(index=layer_index).name in models:
    models[vgg.get_layer(index=layer_index).name] = tf.keras.models.Model(inputs=vgg.input, outputs=vgg.get_layer(index=layer_index).output)
generated_img = [models[layer_name](content_image) for layer_name in models.keys()]

fig, axs = plt.subplots(1,5)
fig.suptitle("None", fontsize=16, )
fig.set_size_inches(20,9)
#axs[0][model].set_title(f"Filter output of layer {layers[model]}")
for i in range(5): 
  axs[i].imshow(np.asarray(generated_img[4][0][:,:,i+15]),cmap="gray")
  axs[i].grid(False)
  axs[i].set_xticks([])
  axs[i].set_yticks([])

fig.savefig("filter_outputs")
"""